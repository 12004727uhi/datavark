{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7EXnt9uUSv5"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "#!pipenv install matplotlib spacy numpy pandas spacy_stanza spacy-transformers\n",
        "!pip install matplotlib spacy numpy pandas spacy_stanza spacy-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg \n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "uesVopp8ZsoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_google_drive(colab=False):\n",
        "    if colab:\n",
        "      from google.colab import drive\n",
        "      drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kJ2KMXaXVBEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QPQc5ddUUSv6"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import spacy, json, copy, glob, os, matplotlib, stanza, spacy_stanza, spacy_transformers\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "\n",
        "def get_annotation_urls(url, ext, print_output=True):\n",
        "    global annotated_files\n",
        "    # ensure url has trailing slash\n",
        "    url = url + '/' if url[-1:] != '/' else url\n",
        "    # load hand annotated examples\n",
        "    annotated_files = glob.glob(url + f'*.{ext}')\n",
        "    # sort based on filename\n",
        "    annotated_files.sort(key=lambda x: os.path.basename(x))\n",
        "    # print counted files to demonstrate success\n",
        "    if print_output:\n",
        "        print(f'Number of annotated files: {len(annotated_files)}')\n",
        "\n",
        "def json_to_doc(print_output=False):\n",
        "    # Load json into list of Python dicts\n",
        "    global annotations\n",
        "    annotations = []\n",
        "    for f in annotated_files:\n",
        "        with open(f, 'r') as file:\n",
        "            annotations.append(json.loads(file.read()))\n",
        "    if print_output:\n",
        "        # print count of annotation dicts to verify success\n",
        "        print(f'Number of annotations in files: {len(annotations)}')\n",
        "        # print first element (document), to verify\n",
        "        print(f'Annotation sample: {annotations[:1]}')\n",
        "\n",
        "def load_models(print_output, model):\n",
        "    global nlp, nlp_trf_orig\n",
        "    # load models\n",
        "    if model == 'stanza':\n",
        "        stanza.download(\"en\")\n",
        "        nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "    elif model in ['trf-model-best', 'trf-model-best-tuned', 'cnn-model-best']:\n",
        "        nlp = spacy.load(trained_model_path + model, exclude='parser,tagger,attribute_ruler,lemmatizer')\n",
        "        nlp_trf_orig = spacy.load('en_core_web_trf' if model in ['trf-model-best', 'trf-model-best-tuned'] else 'en_core_web_lg')  # required as workaround to frozen components bug\n",
        "        nlp.add_pipe('parser', source=nlp_trf_orig, after='transformer' if model in ['trf-model-best', 'trf-model-best-tuned'] else 'tok2vec')\n",
        "        nlp.add_pipe('tagger', source=nlp_trf_orig, after='parser')\n",
        "        nlp.add_pipe('attribute_ruler', source=nlp_trf_orig, after='tagger')\n",
        "        nlp.add_pipe('lemmatizer', source=nlp_trf_orig, after='attribute_ruler')\n",
        "        print(f'Evaluating model: {model}') if print_output else None\n",
        "    else:\n",
        "        model = trained_model_path + model if model not in ['en_core_web_lg', 'en_core_web_trf'] else model\n",
        "        print(f'Evaluating model: {model}') if print_output else None\n",
        "        nlp = spacy.load(model)\n",
        "    \n",
        "\n",
        "def setup(model, dataset, print_output=False, colab=0):\n",
        "    global trained_model_path, doc_results_path, doc_results_filename, corpus_results_path, corpus_results_filename\n",
        "    \"\"\"paths\"\"\"\n",
        "    mount_google_drive(colab)\n",
        "    google_drive_path = '/content/drive/MyDrive/dissertation/training/'\n",
        "    annotations_path = f'{google_drive_path if colab else \"./\"}./experiment_test_files/{dataset}/annotations/'\n",
        "    trained_model_path = f'{google_drive_path if colab else \"./\"}./training_output/'\n",
        "    doc_results_path = f'{google_drive_path if colab else \"./\"}./results/'\n",
        "    doc_results_filename = f'doc_{model}.csv'\n",
        "    corpus_results_path = f'{google_drive_path if colab else \"./\"}./results/'\n",
        "    corpus_results_filename = f'corpus_{model}.csv'\n",
        "    annotations_data_filetype = 'json'\n",
        "    \"\"\"globals set here\"\"\"\n",
        "    global annotated_files, labels_of_interest\n",
        "    # define entity labels of interest\n",
        "    labels_of_interest = ['GPE', 'LOC', 'DATE', 'TIME', 'COLOR', 'TYPE']\n",
        "    # run setup functions\n",
        "    get_annotation_urls(annotations_path, annotations_data_filetype,print_output)\n",
        "    json_to_doc(print_output)\n",
        "    load_models(print_output, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgnX6E4xUSv7"
      },
      "outputs": [],
      "source": [
        "def get_annotated_entities(annotations):\n",
        "    # function to get all entities in a hand-annotated doc (all paras)\n",
        "    return [para[1]['entities'] for para in annotations]\n",
        "\n",
        "def get_text(annotations):\n",
        "    # function to get all raw text from the hand-annotated doc (all paras)\n",
        "    return [para[0] for para in annotations]\n",
        "\n",
        "def get_annotations(print_output=False):\n",
        "    global annotated_entities, annotated_text\n",
        "    \"\"\"Note: Entities to be stored in the form [[[element1, element2]],[[element1, element2]]]\n",
        "    \"\"\"\n",
        "    # run function to get all entities from all paras in all the passed-in hand-annotated docs\n",
        "    annotated_entities = [get_annotated_entities(doc['annotations']) for doc in annotations]\n",
        "    # run function to get all text from all paras in all the passed-in hand-annotated docs\n",
        "    annotated_text = [get_text(doc['annotations']) for doc in annotations]\n",
        "    \"\"\"Note: Annotated text stored in form [[para1, para1],[para1, para2]]\n",
        "    i.e., a list of document-lists of paras\n",
        "    \"\"\"\n",
        "    if print_output:\n",
        "        # print total counts of annotated documents; paras and entities\n",
        "        print(f'Number of documents: {len(annotated_entities)}')\n",
        "        print(f'Number of paras: {sum([len(x) for x in annotated_entities])}')\n",
        "        print(f'Number of entities: {sum([sum(len(y) for y in x ) for x in annotated_entities])}\\n')\n",
        "        # print first entity, of first para, of first doc, to verify entities\n",
        "        print(f'Annotated entities sample (doc 4, para 1): {annotated_entities[3][0]}\\n')\n",
        "         # print sample of annotated text to verify\n",
        "        print(f'Annotated text sample (doc 4, para 1): {annotated_text[3][0]}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wjO5Bpw_USv7"
      },
      "outputs": [],
      "source": [
        "def run_ner(model, print_output=False):\n",
        "    global extracted_entities\n",
        "    extracted_entities = [] # in the form [[[[element1, element2]],[doc2[element1, element2]]]]\n",
        "    # extract using spaCy's CNN NER\n",
        "    for doc_text in annotated_text:\n",
        "        doc_ents = []\n",
        "        for para in nlp.pipe(doc_text, disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer']):\n",
        "            doc_ents.append([[ent.start_char, ent.end_char, ent.label_] for ent in para.ents])  # could also add ent.text\n",
        "        extracted_entities.append(doc_ents)\n",
        "    \n",
        "    if print_output:\n",
        "        # print total counts of processed documents; paras and entities\n",
        "        print(f'Number of documents: {len(extracted_entities)}')\n",
        "        print(f'Number of paras: {sum([len(x) for x in extracted_entities])}')\n",
        "        print(f'Number of entities: {sum([sum(len(y) for y in x ) for x in extracted_entities])}')\n",
        "        print('\\n')\n",
        "        print(f'Extract entities: {extracted_entities}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4UTT0-XUSv8"
      },
      "outputs": [],
      "source": [
        "# function to count annotated and extracted entities in doc\n",
        "def count_entities(doc):\n",
        "    return sum([len(list(ent for ent in para)) for para in doc]) if any(doc) else 0\n",
        "\n",
        "def count_sample_paragraphs():\n",
        "    return sum([len(doc['annotations']) for doc in annotations])\n",
        "\n",
        "def count_entity_types(dataset='annotations'):\n",
        "    global entity_count\n",
        "    entity_count = {k:0 for k in labels_of_interest}\n",
        "    for doc in annotated_entities if dataset == 'annotations' else extracted_entities:\n",
        "        for para in doc:\n",
        "            for ent in para:\n",
        "                entity_count[ent[2]] += 1\n",
        "    return entity_count\n",
        "\n",
        "\"\"\" run the count routines\n",
        "\"\"\"\n",
        "def run_counts(print_output=False):\n",
        "    global doc_extracted_entities_count, doc_annotated_entities_count, corpus_extracted_entities_count, corpus_annotated_entities_count, extracted_entities, annotated_entities, corpus_sample_paras_total\n",
        "    # count all entities for each doc\n",
        "    doc_extracted_entities_count = [count_entities(doc) for doc in extracted_entities]\n",
        "    doc_annotated_entities_count = [count_entities(doc) for doc in annotated_entities]\n",
        "    # count all sample paragraphs for corpus\n",
        "    corpus_sample_paras_total = count_sample_paragraphs()\n",
        "    # count all entities for corpus\n",
        "    corpus_extracted_entities_count = sum(doc_extracted_entities_count)\n",
        "    corpus_annotated_entities_count = sum(doc_annotated_entities_count)\n",
        "    # print output (always)\n",
        "    print(f'\\nExtracted entities count for each doc: {doc_extracted_entities_count}\\nAnnotated entities count for each doc: {doc_annotated_entities_count}')\n",
        "    print(f'\\nTotal annotated sample paragraphs in the corpus: {corpus_sample_paras_total}\\n')\n",
        "    print(f'Extracted entities count for corpus: {corpus_extracted_entities_count}')\n",
        "    print(f'Annotated entities count for corpus: {corpus_annotated_entities_count}\\n')\n",
        "    print(f'Totals of annotated entity types: {count_entity_types(\"annotations\")}')\n",
        "    print(f'Totals of extracted entity types: {count_entity_types(\"ner\")}')\n",
        "    # print output (optional)\n",
        "    if print_output:\n",
        "        print(f'\\nExtracted entities for corpus: {extracted_entities}')\n",
        "        print(f'Annotated entities for corpus: {annotated_entities}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxOB7DthUSv8"
      },
      "outputs": [],
      "source": [
        "# function to remove any entities not of interest in doc (ensures extra detected entities do not influence performance calculations and useful for calculating performance without influence of custom entities)\n",
        "def remove_irrelevant_entities(doc):\n",
        "    for idx, para in enumerate(doc):\n",
        "        doc[idx] = [ent for ent in para if ent[2] in labels_of_interest]\n",
        "    return doc\n",
        "\n",
        "# perform preprocessing on the data\n",
        "def preprocess_data(print_output=False):\n",
        "    global extracted_entities, annotated_entities, annotated_text, test_data\n",
        "    # extract irrelevant entities\n",
        "    extracted_entities = [remove_irrelevant_entities(doc) for doc in extracted_entities]\n",
        "    annotated_entities = [remove_irrelevant_entities(doc) for doc in annotated_entities]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBrl8eeqUSv8"
      },
      "outputs": [],
      "source": [
        "def analyse(print_output=False):\n",
        "\n",
        "    global doc_extracted_entities_count, doc_annotated_entities_count, corpus_extracted_entities_count, corpus_annotated_entities_count, extracted_entities, corpus_false_positive_entities, true_positives, false_negatives, corpus_true_positives_total, doc_results\n",
        "\n",
        "    def find_matches(print_output, doc_idx, annotated, extracted):\n",
        "        # find true positives and false negatives, per document\n",
        "        true_pos = 0\n",
        "        false_neg = doc_annotated_entities_count[doc_idx] - doc_extracted_entities_count[doc_idx]\n",
        "        for para_idx, (a_ents, e_ents) in enumerate(zip(annotated, extracted)):\n",
        "            ee_matched = []\n",
        "            ae_matched = []\n",
        "            # look for extracted entities within annotated entity boundaries (matches)\n",
        "            for ae in a_ents:\n",
        "                for ee in e_ents:\n",
        "                    if (ee[1] >= ae[0] and ee[0] <= ae[1]):\n",
        "                        if ee[2] == ae[2]: # true positive identified!\n",
        "                            if ee in ee_matched: \n",
        "                                false_neg -= 1  # decrement false negatives by 1\n",
        "                            else:\n",
        "                                ee_matched.append(ee)  # record NER identified entity as seen\n",
        "                                true_pos += 1  # increment true positives by 1\n",
        "                            if ae in ae_matched:\n",
        "                                false_neg += 1  # increment false negatives by 1\n",
        "                            else:\n",
        "                                ae_matched.append(ae)  # record appended entity as seen\n",
        "        # Prevent false_neg falling below 0 in event that \n",
        "        #  more NER entities than annotated unduly influence this calculation\n",
        "        false_neg = false_neg if false_neg > 0 else 0           \n",
        "        return true_pos, false_neg\n",
        "\n",
        "    def calc_corpus_true_positives_total(print_output):\n",
        "        global corpus_true_positives_total\n",
        "        corpus_true_positives_total = sum(true_positives)\n",
        "\n",
        "    def calc_corpus_false_negatives_total(print_output):\n",
        "        # function to calculate false negatives for the corpus\n",
        "        global corpus_false_negatives_total\n",
        "        corpus_false_negatives_total = sum(false_negatives)\n",
        "    \n",
        "    def calc_true_pos_false_neg(print_output):\n",
        "        global true_positives, false_negatives\n",
        "        true_positives = []\n",
        "        false_negatives = []\n",
        "        for doc_idx, doc in enumerate(annotated_entities):\n",
        "            true_pos, false_neg = find_matches(print_output, doc_idx, annotated_entities[doc_idx],extracted_entities[doc_idx])\n",
        "            true_positives.append(true_pos)\n",
        "            false_negatives.append(false_neg)\n",
        "\n",
        "    # functions to compute precision, recall and f1-score for model-level evaluation\n",
        "\n",
        "    def model_level_eval_doc():\n",
        "        global doc_results\n",
        "        doc_results = []\n",
        "        precision_list = []\n",
        "        recall_list = []\n",
        "        f1_score_list = []\n",
        "         \n",
        "        for tp,fn,ee in zip(true_positives,false_negatives,doc_extracted_entities_count):\n",
        "            # calculate precision for each doc\n",
        "            precision_list.append(tp/ee) if tp > 0 else precision_list.append(1.0) if (fn == 0 and ee == 0) else precision_list.append(0)\n",
        "             # calculate recall for each doc\n",
        "            recall_list.append(tp/(tp + fn)) if tp > 0 else recall_list.append(1.0) if (fn == 0 and ee == 0) else recall_list.append(0)\n",
        "        for idx, (p,r,tp,fn,ee) in enumerate(zip(precision_list,recall_list,true_positives,false_negatives,doc_extracted_entities_count)):\n",
        "             # calculate f1-score for each doc\n",
        "            f1_score_list.append((2 * p * r / (p + r))) if (p > 0 and r > 0) else f1_score_list.append(1.0) if (tp == 0 and (ee == 0 and fn == 0)) else f1_score_list.append(0)\n",
        "        # add results to results dictionary\n",
        "        for doc in range(len(precision_list)):\n",
        "            doc_results.append({\n",
        "                'precision': round(precision_list[doc], 3), \n",
        "                'recall': round(recall_list[doc], 3),\n",
        "                'f1-score': round(f1_score_list[doc], 3)\n",
        "                })\n",
        "\n",
        "    def model_level_eval_corpus():\n",
        "        global corpus_results\n",
        "        # calculate precision of corpus\n",
        "        precision = corpus_true_positives_total/corpus_extracted_entities_count if corpus_true_positives_total >  0 else 1.0 if (corpus_extracted_entities_count == 0 and corpus_false_negatives_total == 0) else 0\n",
        "        # calculate recall for each doc\n",
        "        recall = corpus_true_positives_total/(corpus_true_positives_total + corpus_false_negatives_total) if corpus_true_positives_total > 0 else 1.0 if (corpus_extracted_entities_count == 0 and corpus_false_negatives_total == 0) else 0\n",
        "        # calculate f1-score for each doc\n",
        "        f1_score = 2 * precision * recall / (precision + recall) if (precision > 0 and recall > 0) else 1.0 if corpus_true_positives_total == 0 and (corpus_extracted_entities_count == 0 and corpus_false_negatives_total == 0) else 0\n",
        "        corpus_results = {\n",
        "            'precision': round(precision, 3), \n",
        "            'recall': round(recall, 3), \n",
        "            'f1_score':round(f1_score, 3)\n",
        "            }\n",
        "    \n",
        "    # run the tasks\n",
        "    corpus_false_positive_entities = []\n",
        "    corpus_missed_entities = []\n",
        "    corpus_false_positive_entities.clear() # clear list\n",
        "    corpus_missed_entities.clear() # clear list first\n",
        "    preprocess_data(print_output=0)  # create working copies of data\n",
        "    run_counts(0)\n",
        "    calc_true_pos_false_neg(print_output)\n",
        "    calc_corpus_true_positives_total(print_output)\n",
        "    calc_corpus_false_negatives_total(print_output)\n",
        "    model_level_eval_doc()\n",
        "    model_level_eval_corpus()\n",
        "\n",
        "    if print_output:\n",
        "        print('\\n')\n",
        "        # print totals per doc\n",
        "        for doc_idx, doc in enumerate(extracted_entities):\n",
        "            print(f'Document {doc_idx}')\n",
        "            print(f'True positives: {true_positives[doc_idx]}')\n",
        "            print(f'False positives: {doc_extracted_entities_count[doc_idx] - true_positives[doc_idx]}')\n",
        "            print(f'False negatives: {false_negatives[doc_idx]}\\n')\n",
        "        # print total for all docs\n",
        "        print(f'\\nDocument analysis results: {doc_results}')\n",
        "        print(f'Corpus analysis results {corpus_results}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbOueK6gUSv9"
      },
      "outputs": [],
      "source": [
        "def display_examples(docs_of_interest):\n",
        "    for doc in docs_of_interest:\n",
        "        try:\n",
        "            for para_idx, para in enumerate(annotated_entities[doc]):\n",
        "                ae = annotated_entities[doc][para_idx]\n",
        "                text = annotated_text[doc][para_idx]\n",
        "                print(f'\\nAnnotated example for document {doc}')\n",
        "                displacy.render({\n",
        "                'text': text,\n",
        "                'ents': [{\"start\": e[0], \"end\": e[1], \"label\": e[2]} for e in ae],\n",
        "                \"title\": f'Document {doc}, para {para_idx}'\n",
        "            }, manual=True, style='ent', jupyter=True)\n",
        "                print(f'Extracted example for document {doc}')\n",
        "                ee = extracted_entities[doc][para_idx]\n",
        "                displacy.render({\n",
        "                'text': text,\n",
        "                'ents': [{\"start\": e[0], \"end\": e[1], \"label\": e[2]} for e in ee],\n",
        "                \"title\": f'Document {doc}, para {para_idx}'\n",
        "            }, manual=True, style='ent', jupyter=True)\n",
        "        except IndexError:\n",
        "            print(f'You appear to be trying to display results for document {doc}, which does not appear to exist!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFGW4K0kUSv-"
      },
      "outputs": [],
      "source": [
        "def display_results(show_label_examples=[], show_scores=True):\n",
        "    global df_docs_results, df_corpus_results\n",
        "    if show_label_examples:\n",
        "        display_examples(show_label_examples)\n",
        "    if show_scores:\n",
        "        # convert dict to pandas dataframe, for display\n",
        "        dr = dict()\n",
        "        for idx, doc in enumerate(doc_results):\n",
        "            dr[idx] = {k.capitalize(): v for k,v in doc.items()}\n",
        "        print('\\nDocument Analysis Results')\n",
        "        df_docs_results = pd.DataFrame.from_dict(dr)\n",
        "        df_docs_results = df_docs_results.T\n",
        "        df_docs_results = df_docs_results[['Precision','Recall','F1-score']]\n",
        "        df_docs_results.index.name = 'Document'\n",
        "        display(df_docs_results)\n",
        "        print('\\nCorpus Analysis Results')\n",
        "        cr = {k.capitalize():[v] for k, v in corpus_results.items()}\n",
        "        df_corpus_results = pd.DataFrame.from_dict(cr)\n",
        "        df_corpus_results.index.name = 'Corpus'\n",
        "        display(df_corpus_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWka_K6eUSv-"
      },
      "outputs": [],
      "source": [
        "def write_results_to_file(write):\n",
        "    if write:\n",
        "        df_docs_results.to_csv(doc_results_path + doc_results_filename)\n",
        "        df_corpus_results.to_csv(corpus_results_path + corpus_results_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0Bz7V9wMUSv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c92799-fc00-4725-fc69-69388e9848b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "global model\n",
        "'''run processes'''\n",
        "model = 'cnn-model-best' # model options: en_core_web_lg, en_core_web_trf, stanza, and custom (e.g., trained)\n",
        "setup(model=model, dataset='test', print_output=0, colab=1)  # dataset from 'train', 'test'\n",
        "get_annotations(print_output=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_ner(model, print_output=0)"
      ],
      "metadata": {
        "id": "o1h6oP07Xal8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyse(print_output=0) \n",
        "# args: ([Indexes (counting from 1) of docs to display entities for (list)], show scores (boolean))\n",
        "display_results([4],1)\n",
        "write_results_to_file(1)"
      ],
      "metadata": {
        "id": "Q3NWbPf7Xa6g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f8bd2e9-e9c5-4644-9309-05a820f4250a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted entities count for each doc: [3, 1, 1, 8, 1, 8, 4, 5, 9, 12, 8, 4, 5, 7, 4, 14, 7, 4, 7, 4]\n",
            "Annotated entities count for each doc: [3, 2, 2, 5, 3, 8, 4, 5, 11, 11, 9, 5, 5, 11, 4, 9, 11, 5, 6, 5]\n",
            "\n",
            "Total annotated sample paragraphs in the corpus: 42\n",
            "\n",
            "Extracted entities count for corpus: 116\n",
            "Annotated entities count for corpus: 124\n",
            "\n",
            "Totals of annotated entity types: {'GPE': 28, 'LOC': 6, 'DATE': 11, 'TIME': 24, 'COLOR': 22, 'TYPE': 33}\n",
            "Totals of extracted entity types: {'GPE': 36, 'LOC': 0, 'DATE': 12, 'TIME': 25, 'COLOR': 19, 'TYPE': 24}\n",
            "\n",
            "Annotated example for document 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">Document 4, para 0</h2>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Had just watched the ISS go across the sky and was looking for a different rocket when I noticed a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    dark\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COLOR</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    spherical\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TYPE</span>\n",
              "</mark>\n",
              " object zipping along. The object appeared to be significantly lower in altitude than a jet, and was going very fast - but slow enough for me to watch it travel in a straight line for \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    maybe ten seconds.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              " It didn’t make any noise. I believe it was traveling south to north, or at least heading north-ish. My sense of direction isn’t great, haha.</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted example for document 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">Document 4, para 0</h2>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Had just watched the ISS go across the sky and was looking for a different rocket when I noticed a dark, spherical object zipping along. The object appeared to be significantly lower in altitude than a jet, and was going very fast - but slow enough for me to watch it travel in a straight line for maybe \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ten seconds.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              " It didn’t make any noise. I believe it was traveling south to north, or at least heading north-ish. My sense of direction isn’t great, haha.</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Analysis Results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          Precision  Recall  F1-score\n",
              "Document                             \n",
              "0             1.000   1.000     1.000\n",
              "1             1.000   0.500     0.667\n",
              "2             1.000   0.500     0.667\n",
              "3             0.625   1.000     0.769\n",
              "4             1.000   0.333     0.500\n",
              "5             1.000   1.000     1.000\n",
              "6             1.000   1.000     1.000\n",
              "7             1.000   1.000     1.000\n",
              "8             1.000   0.818     0.900\n",
              "9             0.917   1.000     0.957\n",
              "10            1.000   0.889     0.941\n",
              "11            1.000   0.800     0.889\n",
              "12            1.000   1.000     1.000\n",
              "13            0.143   0.200     0.167\n",
              "14            0.750   1.000     0.857\n",
              "15            0.643   1.000     0.783\n",
              "16            0.857   0.545     0.667\n",
              "17            0.750   0.750     0.750\n",
              "18            0.714   1.000     0.833\n",
              "19            0.750   0.750     0.750"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8da5cf8-8869-4f14-9d0c-cfc41a848a9d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Document</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.625</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.818</td>\n",
              "      <td>0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.917</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.889</td>\n",
              "      <td>0.941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.143</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.750</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.643</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.857</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.714</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8da5cf8-8869-4f14-9d0c-cfc41a848a9d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8da5cf8-8869-4f14-9d0c-cfc41a848a9d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8da5cf8-8869-4f14-9d0c-cfc41a848a9d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus Analysis Results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Precision  Recall  F1_score\n",
              "Corpus                             \n",
              "0           0.819   0.833     0.826"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da577c57-7568-483a-b8af-0d5eba28e686\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Corpus</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.819</td>\n",
              "      <td>0.833</td>\n",
              "      <td>0.826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da577c57-7568-483a-b8af-0d5eba28e686')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da577c57-7568-483a-b8af-0d5eba28e686 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da577c57-7568-483a-b8af-0d5eba28e686');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# semantic test\n",
        "def print_ents(doc):\n",
        "  if doc.ents:\n",
        "    for ent in doc.ents:\n",
        "      print(f'Text: {ent.text}')\n",
        "      print(f'Label: {ent.label_}')\n",
        "  else:\n",
        "    print('None')\n",
        "token = \"disk\"\n",
        "token_in_context = f'I saw an object in the sky, it looked like a {token}'\n",
        "token_in_wrong_context = f'I played a {token} on my CD player!'\n",
        "print(f'Token absent context: ')\n",
        "print_ents(nlp(token))\n",
        "print(f'\\nToken in correct context: ')\n",
        "print_ents(nlp(token_in_context))\n",
        "print(f'\\nToken in incorrect context: ')\n",
        "print_ents(nlp(token_in_wrong_context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4MQN_sQU5BP",
        "outputId": "d3220eca-83fe-4d8e-8f7d-4c045cad13ce"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token absent context: \n",
            "Text: disk\n",
            "Label: TYPE\n",
            "\n",
            "Token in correct context: \n",
            "Text: disk\n",
            "Label: TYPE\n",
            "\n",
            "Token in incorrect context: \n",
            "None\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.14 ('uhi-ZQVV2iWc')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "65f88eacf3a10b22e2367da6754b23494b2804a02fe03c23afbe72788a968f5d"
      }
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}